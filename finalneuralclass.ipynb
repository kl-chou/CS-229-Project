{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#parse data \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#label encoding on categorical data \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#FAMA 49CRSP Common Stocks \n",
    "df = pd.read_csv('../ee6d2f60cdafb550.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25 25 25 ... 22 10 47]\n",
      "       public_date  FFI49_desc  NFIRM  indret_ew  indret_vw  dpr_Median  \\\n",
      "12337     19930831          25      6   0.052904   0.068678       0.460   \n",
      "12383     19930930          25      6  -0.035253  -0.046242       0.460   \n",
      "12430     19931031          25      6   0.009319  -0.018318       0.460   \n",
      "12477     19931130          25      5  -0.022123  -0.014000       0.479   \n",
      "12524     19931231          25      5   0.096039   0.072342       0.479   \n",
      "12571     19940131          25      5   0.043779   0.009888       0.479   \n",
      "12618     19940228          25      6   0.012589   0.039537       0.394   \n",
      "12665     19940331          25      6  -0.012399  -0.052069       0.394   \n",
      "12712     19940430          25      6  -0.026779  -0.024747       0.394   \n",
      "12759     19940531          25      6  -0.001201   0.023906       0.394   \n",
      "12806     19940630          25      6  -0.001294  -0.037881       0.394   \n",
      "12853     19940731          25      6   0.023423   0.043118       0.394   \n",
      "12900     19940831          25      6   0.066949   0.055253       0.398   \n",
      "12947     19940930          25      6   0.004471  -0.026728       0.398   \n",
      "12994     19941031          25      6  -0.044276  -0.015566       0.398   \n",
      "13041     19941130          25      6  -0.040811  -0.035578       0.390   \n",
      "13088     19941231          25      6   0.007111   0.043045       0.390   \n",
      "13135     19950131          25      6   0.021079   0.023549       0.390   \n",
      "13182     19950228          25      6   0.058941   0.058359       0.422   \n",
      "13229     19950331          25      6   0.057380   0.023958       0.422   \n",
      "13276     19950430          25      6   0.062681   0.029973       0.422   \n",
      "13323     19950531          25      6   0.037249   0.031139       0.378   \n",
      "13370     19950630          25      6   0.056587   0.052070       0.378   \n",
      "13417     19950731          25      6   0.004115  -0.005009       0.378   \n",
      "13464     19950831          25      6  -0.000317   0.011758       0.422   \n",
      "13511     19950930          25      6   0.053875   0.015358       0.422   \n",
      "13558     19951031          25      6   0.004551  -0.003330       0.422   \n",
      "13605     19951130          25      6   0.047691   0.086727       0.409   \n",
      "13652     19951231          25      6   0.026102   0.035340       0.409   \n",
      "13699     19960131          25      6   0.044772   0.033413       0.409   \n",
      "...            ...         ...    ...        ...        ...         ...   \n",
      "25133     20161231          38      3   0.053255   0.054318       0.851   \n",
      "25134     20161231          39      3   0.023255   0.024929       0.437   \n",
      "25135     20161231          40     28  -0.009651   0.000257       0.000   \n",
      "25136     20161231          41      1  -0.036863  -0.036863       0.840   \n",
      "25137     20161231          42     15   0.020397   0.053427       0.291   \n",
      "25138     20161231          43      3  -0.066658  -0.075610       0.454   \n",
      "25139     20161231          44     15   0.007842   0.007611       0.294   \n",
      "25124     20161231          27      6  -0.004840  -0.002528       0.551   \n",
      "25132     20161231          35     34  -0.025950  -0.006974       0.297   \n",
      "25123     20161231          26     12  -0.002451  -0.008172       0.401   \n",
      "25110     20161231          12      6  -0.015227  -0.017383       0.087   \n",
      "25120     20161231          23     10   0.015469   0.015504       0.443   \n",
      "25141     20161231          46     33   0.036605   0.036090       0.641   \n",
      "25098     20161231           0      6   0.013718   0.016368       0.273   \n",
      "25100     20161231           2      5   0.037148   0.024898       0.232   \n",
      "25101     20161231           3     28   0.044543   0.044528       0.377   \n",
      "25102     20161231           4      4   0.010696   0.033009       0.425   \n",
      "25103     20161231           5      5  -0.004710  -0.006343       0.274   \n",
      "25106     20161231           8     17   0.001487   0.000991       0.316   \n",
      "25122     20161231          25     12   0.005462   0.005478       0.162   \n",
      "25107     20161231           9     12   0.008697   0.006098       0.484   \n",
      "25109     20161231          11      7  -0.050234  -0.012120       0.274   \n",
      "25111     20161231          14     16   0.003128   0.009955       0.375   \n",
      "25113     20161231          16     13   0.023076   0.036328       0.370   \n",
      "25114     20161231          17     14   0.046219   0.053876       0.616   \n",
      "25117     20161231          20      1  -0.057719  -0.057719       0.560   \n",
      "25118     20161231          21      6   0.018384   0.012505       0.510   \n",
      "25119     20161231          22      6  -0.012955   0.001926       0.026   \n",
      "25108     20161231          10     20   0.014731   0.029014       0.417   \n",
      "25142     20161231          47      9   0.006330   0.004922       0.392   \n",
      "\n",
      "       PEG_trailing_Median  bm_Median  CAPEI_Median  divyield_Median  \\\n",
      "12337                4.556      0.438        21.835          0.02290   \n",
      "12383                4.380      0.438        20.573          0.02450   \n",
      "12430                4.268      0.438        20.959          0.02510   \n",
      "12477                3.865      0.401        19.861          0.02730   \n",
      "12524                4.122      0.401        21.179          0.02550   \n",
      "12571                4.190      0.401        21.483          0.02120   \n",
      "12618               11.991      0.406        22.148          0.02340   \n",
      "12665               11.971      0.406        20.340          0.02330   \n",
      "12712               11.623      0.406        19.663          0.02510   \n",
      "12759                3.700      0.393        19.092          0.02450   \n",
      "12806                3.833      0.393        18.900          0.02520   \n",
      "12853                3.825      0.393        19.442          0.02480   \n",
      "12900                1.836      0.397        19.539          0.02390   \n",
      "12947                1.765      0.397        18.768          0.02390   \n",
      "12994                1.664      0.397        18.101          0.02440   \n",
      "13041                1.371      0.382        17.360          0.02690   \n",
      "13088                1.447      0.382        18.339          0.02710   \n",
      "13135                1.376      0.382        18.007          0.02600   \n",
      "13182                1.572      0.390        19.144          0.02490   \n",
      "13229                1.628      0.390        19.835          0.02460   \n",
      "13276                1.653      0.395        25.704          0.02370   \n",
      "13323                1.430      0.359        26.911          0.02230   \n",
      "13370                1.520      0.359        27.939          0.02120   \n",
      "13417                1.564      0.359        28.178          0.02170   \n",
      "13464                1.358      0.334        27.136          0.02140   \n",
      "13511                1.124      0.334        27.864          0.02120   \n",
      "13558                1.254      0.334        27.607          0.02160   \n",
      "13605                1.052      0.348        28.600          0.02040   \n",
      "13652                1.076      0.348        30.916          0.01970   \n",
      "13699                1.046      0.339        31.812          0.01740   \n",
      "...                    ...        ...           ...              ...   \n",
      "25133                5.987      0.273        28.409          0.03610   \n",
      "25134                0.997      0.166        24.194          0.02860   \n",
      "25135                1.724      0.177        27.234          0.01410   \n",
      "25136                3.066      0.523        33.958          0.02540   \n",
      "25137                0.765      0.502        22.358          0.01970   \n",
      "25138                0.611      0.233        25.309          0.02620   \n",
      "25139                1.132      0.432        21.286          0.01530   \n",
      "25124                1.497      0.101        31.726          0.03080   \n",
      "25132                1.589      0.305        21.762          0.02060   \n",
      "25123                0.807      0.312        16.528          0.02020   \n",
      "25110                0.500      0.655        14.286          0.01530   \n",
      "25120                3.701      0.212        26.668          0.02390   \n",
      "25141                2.104      0.798        23.026          0.03350   \n",
      "25098                1.627      0.241        22.032          0.01760   \n",
      "25100                0.446      0.542         9.784          0.02440   \n",
      "25101                2.449      0.877        15.996          0.01820   \n",
      "25102                8.059      0.185        26.059          0.01660   \n",
      "25103                0.568      0.293        41.512          0.01660   \n",
      "25106                1.826      0.219        32.407          0.01580   \n",
      "25122                2.457      0.265        23.877          0.00655   \n",
      "25107                1.148      0.295        24.150          0.02280   \n",
      "25109                0.845      0.210        19.595          0.01820   \n",
      "25111                1.298      0.178        24.618          0.02740   \n",
      "25113                1.429      0.596        22.645          0.02020   \n",
      "25114                1.332      0.219        25.145          0.02340   \n",
      "25117                2.528      0.033        23.666          0.02910   \n",
      "25118                2.234      0.411        18.186          0.03190   \n",
      "25119                1.222      0.396        21.419          0.01060   \n",
      "25108                1.664      0.262        24.592          0.02060   \n",
      "25142                3.060      0.244        22.458          0.02240   \n",
      "\n",
      "               ...            rect_turn_Median  sale_equity_Median  \\\n",
      "12337          ...                       5.494               3.146   \n",
      "12383          ...                       5.494               3.146   \n",
      "12430          ...                       5.685               3.027   \n",
      "12477          ...                       5.364               2.755   \n",
      "12524          ...                       5.364               2.755   \n",
      "12571          ...                       5.319               2.795   \n",
      "12618          ...                       4.789               2.451   \n",
      "12665          ...                       4.789               2.451   \n",
      "12712          ...                       4.789               2.525   \n",
      "12759          ...                       4.704               2.516   \n",
      "12806          ...                       4.704               2.516   \n",
      "12853          ...                       4.704               2.448   \n",
      "12900          ...                       4.589               2.449   \n",
      "12947          ...                       4.589               2.449   \n",
      "12994          ...                       4.589               2.513   \n",
      "13041          ...                       4.454               2.458   \n",
      "13088          ...                       4.454               2.458   \n",
      "13135          ...                       4.454               2.435   \n",
      "13182          ...                       4.443               2.568   \n",
      "13229          ...                       4.443               2.568   \n",
      "13276          ...                       4.443               2.526   \n",
      "13323          ...                       4.521               2.463   \n",
      "13370          ...                       4.521               2.463   \n",
      "13417          ...                       4.521               2.282   \n",
      "13464          ...                       4.551               2.364   \n",
      "13511          ...                       4.551               2.364   \n",
      "13558          ...                       4.551               2.510   \n",
      "13605          ...                       4.621               2.689   \n",
      "13652          ...                       4.621               2.689   \n",
      "13699          ...                       4.621               2.692   \n",
      "...            ...                         ...                 ...   \n",
      "25133          ...                      84.444               3.603   \n",
      "25134          ...                       9.870               1.646   \n",
      "25135          ...                       6.467               1.324   \n",
      "25136          ...                       9.640               2.084   \n",
      "25137          ...                       6.513               1.331   \n",
      "25138          ...                       5.333               2.850   \n",
      "25139          ...                      11.577               3.411   \n",
      "25124          ...                      24.406               3.634   \n",
      "25132          ...                      57.162               4.892   \n",
      "25123          ...                       4.811               1.908   \n",
      "25110          ...                      15.815               2.066   \n",
      "25120          ...                       7.705               3.133   \n",
      "25141          ...                       9.204               0.835   \n",
      "25098          ...                       6.365               2.672   \n",
      "25100          ...                       5.036               3.614   \n",
      "25101          ...                       0.092               0.422   \n",
      "25102          ...                       6.947               1.606   \n",
      "25103          ...                       7.212               1.779   \n",
      "25106          ...                       4.239               2.062   \n",
      "25122          ...                       6.038               1.086   \n",
      "25107          ...                       5.477               1.787   \n",
      "25109          ...                       9.522               2.378   \n",
      "25111          ...                       5.736               1.169   \n",
      "25113          ...                       4.244               0.499   \n",
      "25114          ...                      11.066               2.545   \n",
      "25117          ...                       5.158              15.531   \n",
      "25118          ...                       5.850               1.397   \n",
      "25119          ...                       7.180               1.817   \n",
      "25108          ...                       7.178               0.982   \n",
      "25142          ...                       9.550               4.712   \n",
      "\n",
      "       sale_invcap_Median  sale_nwc_Median  accrual_Median  rd_sale_Median  \\\n",
      "12337               2.601            9.285           0.049           0.068   \n",
      "12383               2.601            9.285           0.056           0.042   \n",
      "12430               2.484            9.285           0.056           0.042   \n",
      "12477               2.345            8.303           0.057           0.080   \n",
      "12524               2.345            8.303           0.057           0.080   \n",
      "12571               2.347            8.303           0.057           0.080   \n",
      "12618               2.121            7.341           0.038           0.068   \n",
      "12665               2.121            7.341           0.038           0.068   \n",
      "12712               2.163            7.321           0.038           0.068   \n",
      "12759               2.165            7.181           0.029           0.066   \n",
      "12806               2.165            7.181           0.029           0.066   \n",
      "12853               2.098            6.644           0.018           0.066   \n",
      "12900               2.110            6.244           0.001           0.064   \n",
      "12947               2.110            6.244           0.001           0.064   \n",
      "12994               2.162            6.483          -0.012           0.064   \n",
      "13041               2.132            6.226           0.012           0.063   \n",
      "13088               2.132            6.226           0.012           0.063   \n",
      "13135               2.121            6.171           0.012           0.063   \n",
      "13182               2.167            5.887           0.018           0.061   \n",
      "13229               2.167            5.887           0.018           0.061   \n",
      "13276               2.141            5.817           0.018           0.061   \n",
      "13323               2.157            5.438           0.016           0.059   \n",
      "13370               2.157            5.438           0.016           0.059   \n",
      "13417               2.032            5.562           0.016           0.059   \n",
      "13464               2.043            5.071           0.012           0.058   \n",
      "13511               2.043            5.071           0.012           0.058   \n",
      "13558               2.141            5.032           0.012           0.058   \n",
      "13605               2.123            5.813           0.007           0.056   \n",
      "13652               2.123            5.813           0.007           0.056   \n",
      "13699               2.116            5.819           0.007           0.056   \n",
      "...                   ...              ...             ...             ...   \n",
      "25133               1.202           37.073           0.000           0.010   \n",
      "25134               0.771            5.962           0.010           0.000   \n",
      "25135               0.848            2.536           0.092           0.142   \n",
      "25136               1.283            3.447           0.067           0.000   \n",
      "25137               0.642            5.943           0.029           0.000   \n",
      "25138               1.539            4.376           0.042           0.053   \n",
      "25139               1.975           22.973           0.054           0.000   \n",
      "25124               2.345           16.491           0.096           0.000   \n",
      "25132               2.979           11.434           0.064           0.000   \n",
      "25123               1.083            3.513           0.044           0.027   \n",
      "25110               1.686            9.334           0.022           0.000   \n",
      "25120               1.532           22.271           0.045           0.018   \n",
      "25141               0.412           28.583           0.048           0.000   \n",
      "25098               1.552            6.191           0.004           0.042   \n",
      "25100               1.527           10.102           0.052           0.038   \n",
      "25101               0.242            2.369           0.005           0.000   \n",
      "25102               0.768            4.789           0.023           0.000   \n",
      "25103               1.288            5.502           0.033           0.010   \n",
      "25106               0.837            8.357           0.043           0.000   \n",
      "25122               0.717            5.891           0.027           0.056   \n",
      "25107               0.819            5.277           0.045           0.028   \n",
      "25109               1.681            4.177           0.023           0.000   \n",
      "25111               0.771            2.366           0.035           0.174   \n",
      "25113               0.336            7.101           0.007           0.000   \n",
      "25114               1.555           12.893           0.039           0.007   \n",
      "25117               2.612           29.236           0.046           0.018   \n",
      "25118               0.955            2.551           0.032           0.130   \n",
      "25119               0.932           13.276           0.053           0.000   \n",
      "25108               0.838            2.092           0.052           0.099   \n",
      "25142               4.343            9.503           0.040           0.000   \n",
      "\n",
      "       adv_sale_Median  staff_sale_Median  PEG_1yrforward_Median  \\\n",
      "12337            0.013              0.000                 -0.333   \n",
      "12383            0.013              0.000                 -0.320   \n",
      "12430            0.013              0.000                 -0.350   \n",
      "12477            0.016              0.000                  0.677   \n",
      "12524            0.016              0.000                  0.722   \n",
      "12571            0.016              0.000                  0.814   \n",
      "12618            0.011              0.000                  1.108   \n",
      "12665            0.011              0.000                  1.013   \n",
      "12712            0.011              0.000                  0.897   \n",
      "12759            0.011              0.000                  0.816   \n",
      "12806            0.011              0.000                  0.866   \n",
      "12853            0.011              0.000                  1.522   \n",
      "12900            0.012              0.000                  1.517   \n",
      "12947            0.012              0.000                  1.456   \n",
      "12994            0.012              0.000                  0.806   \n",
      "13041            0.008              0.000                  0.923   \n",
      "13088            0.008              0.000                  0.975   \n",
      "13135            0.008              0.000                  1.063   \n",
      "13182            0.000              0.000                  1.046   \n",
      "13229            0.000              0.000                  1.123   \n",
      "13276            0.000              0.000                  1.136   \n",
      "13323            0.000              0.000                  1.055   \n",
      "13370            0.000              0.000                  1.134   \n",
      "13417            0.000              0.000                  1.098   \n",
      "13464            0.000              0.000                  1.022   \n",
      "13511            0.000              0.000                  1.002   \n",
      "13558            0.000              0.000                  0.935   \n",
      "13605            0.000              0.000                  0.873   \n",
      "13652            0.000              0.000                  0.863   \n",
      "13699            0.000              0.000                  0.840   \n",
      "...                ...                ...                    ...   \n",
      "25133            0.013              0.000                  3.095   \n",
      "25134            0.078              0.000                  0.567   \n",
      "25135            0.017              0.000                  1.762   \n",
      "25136            0.000              0.000                  1.132   \n",
      "25137            0.040              0.000                  0.458   \n",
      "25138            0.092              0.000                  8.788   \n",
      "25139            0.000              0.245                  2.168   \n",
      "25124            0.022              0.087                  1.605   \n",
      "25132            0.011              0.000                  1.195   \n",
      "25123            0.000              0.000                 -0.713   \n",
      "25110            0.002              0.000                  0.931   \n",
      "25120            0.056              0.000                  3.005   \n",
      "25141            0.000              0.000                  2.240   \n",
      "25098            0.000              0.000                  2.150   \n",
      "25100            0.023              0.000                  0.164   \n",
      "25101            0.015              0.306                  1.301   \n",
      "25102            0.092              0.000                  2.981   \n",
      "25103            0.000              0.000                  0.718   \n",
      "25106            0.000              0.000                  2.369   \n",
      "25122            0.000              0.000                  2.091   \n",
      "25107            0.000              0.000                  1.718   \n",
      "25109            0.047              0.000                  2.371   \n",
      "25111            0.014              0.000                  1.328   \n",
      "25113            0.000              0.242                  1.433   \n",
      "25114            0.029              0.000                  2.894   \n",
      "25117            0.000              0.000                  2.765   \n",
      "25118            0.004              0.000                  1.552   \n",
      "25119            0.000              0.228                  1.508   \n",
      "25108            0.000              0.000                  1.713   \n",
      "25142            0.000              0.000                 -4.237   \n",
      "\n",
      "       PEG_ltgforward_Median  \n",
      "12337                  1.572  \n",
      "12383                  1.445  \n",
      "12430                  1.422  \n",
      "12477                  1.848  \n",
      "12524                  1.962  \n",
      "12571                  1.930  \n",
      "12618                  1.721  \n",
      "12665                  1.587  \n",
      "12712                  1.557  \n",
      "12759                  1.410  \n",
      "12806                  1.438  \n",
      "12853                  1.525  \n",
      "12900                  1.538  \n",
      "12947                  1.524  \n",
      "12994                  1.418  \n",
      "13041                  1.333  \n",
      "13088                  1.429  \n",
      "13135                  1.478  \n",
      "13182                  1.517  \n",
      "13229                  1.610  \n",
      "13276                  1.671  \n",
      "13323                  1.668  \n",
      "13370                  1.722  \n",
      "13417                  1.679  \n",
      "13464                  1.563  \n",
      "13511                  1.687  \n",
      "13558                  1.653  \n",
      "13605                  1.633  \n",
      "13652                  1.570  \n",
      "13699                  1.565  \n",
      "...                      ...  \n",
      "25133                  3.081  \n",
      "25134                  1.997  \n",
      "25135                  2.425  \n",
      "25136                  1.401  \n",
      "25137                  1.019  \n",
      "25138                  2.122  \n",
      "25139                  2.081  \n",
      "25124                  1.987  \n",
      "25132                  1.778  \n",
      "25123                  2.365  \n",
      "25110                  1.580  \n",
      "25120                  3.063  \n",
      "25141                  3.676  \n",
      "25098                  2.645  \n",
      "25100                 -1.294  \n",
      "25101                  1.945  \n",
      "25102                  3.205  \n",
      "25103                  1.579  \n",
      "25106                  2.179  \n",
      "25122                  2.545  \n",
      "25107                  2.572  \n",
      "25109                  2.236  \n",
      "25111                  2.379  \n",
      "25113                  1.546  \n",
      "25114                  3.445  \n",
      "25117                  2.721  \n",
      "25118                  1.875  \n",
      "25119                  1.884  \n",
      "25108                  2.218  \n",
      "25142                  2.072  \n",
      "\n",
      "[9297 rows x 76 columns]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing \n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#preprocessing here\n",
    "#sort by date \n",
    "df = df.sort_values(by = 'public_date', ascending = True)\n",
    "\n",
    "#encode integer categories into numbers \n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(df.FFI49_desc)\n",
    "print(integer_encoded)\n",
    "df.FFI49_desc = integer_encoded\n",
    "df.divyield_Median = [float(x.strip('%'))/100 for x in df.divyield_Median]\n",
    "print(df)\n",
    "\n",
    "#todo: https://www.gsb.stanford.edu/library/articles/databases/links/financial-ratios-suite?fbclid=IwAR0EGNGk9DdxQjEHfdaoUhdY3tNzAWDogYDzuuJi1zT_muL-uJtWQw19Fzk\n",
    "\n",
    "#get output first \n",
    "ewlabels = df.indret_ew\n",
    "vwlabels = df.indret_vw\n",
    "\n",
    "df = df.drop(columns = ['indret_ew','indret_vw'])\n",
    "#3year on year change as a prediction feature, raw pct change \n",
    "yoythree = ewlabels.diff(periods = 3)\n",
    "#3 years rolling percent change, averaged ie. (y1-y2 + (y3-y2)change)/2 \n",
    "rollavgpct = ewlabels.rolling(3).mean()\n",
    "\n",
    "#drop first 3 years\n",
    "df = df.iloc[3:]\n",
    "ewlabels = ewlabels.iloc[3:]\n",
    "yoythree = yoythree.iloc[3:]\n",
    "#yoypctthree = yoypctthree.iloc[3:]\n",
    "rollavgpct = rollavgpct.iloc[3:]\n",
    "\n",
    "#add -1 and 1 so the bins will take on bins to be equal and set to max -1 and 1\n",
    "extrema = pd.Series([-1,1])\n",
    "ewnlabels = ewlabels.append(extrema)\n",
    "\n",
    "#make a new output (bucket by percentage?)\n",
    "enc = KBinsDiscretizer(n_bins=8, encode='ordinal',strategy = 'uniform')\n",
    "ewnlabels = np.asarray(ewnlabels)\n",
    "ewnlabels = ewnlabels.reshape((-1,1))\n",
    "labels_binned = enc.fit_transform(ewnlabels)\n",
    "\n",
    "labels_binned = labels_binned[:-2]\n",
    "\n",
    "#1 Split-Timer series data, 0.64 Train, 0.16 dev, 0.2 Test\n",
    "x_train, x_test, y_train, y_test = train_test_split(df, labels_binned, test_size = 0.2, shuffle = False)\n",
    "x_tra, x_dev, y_tra, y_dev = train_test_split(x_train, y_train, test_size = 0.2, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5948/5948 [==============================] - 1s 92us/step - loss: 1.1545 - acc: 0.5249\n",
      "Epoch 2/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.8009 - acc: 0.5642\n",
      "Epoch 3/50\n",
      "5948/5948 [==============================] - 0s 44us/step - loss: 0.7720 - acc: 0.5642\n",
      "Epoch 4/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7629 - acc: 0.5642\n",
      "Epoch 5/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7590 - acc: 0.5642\n",
      "Epoch 6/50\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7573 - acc: 0.5642\n",
      "Epoch 7/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7552 - acc: 0.5642\n",
      "Epoch 8/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7542 - acc: 0.5642\n",
      "Epoch 9/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7543 - acc: 0.5642\n",
      "Epoch 10/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7533 - acc: 0.5642\n",
      "Epoch 11/50\n",
      "5948/5948 [==============================] - 0s 71us/step - loss: 0.7533 - acc: 0.5642\n",
      "Epoch 12/50\n",
      "5948/5948 [==============================] - 0s 74us/step - loss: 0.7529 - acc: 0.5642\n",
      "Epoch 13/50\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7526 - acc: 0.5642\n",
      "Epoch 14/50\n",
      "5948/5948 [==============================] - 0s 60us/step - loss: 0.7527 - acc: 0.5642\n",
      "Epoch 15/50\n",
      "5948/5948 [==============================] - 0s 52us/step - loss: 0.7522 - acc: 0.5642\n",
      "Epoch 16/50\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7521 - acc: 0.5642\n",
      "Epoch 17/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7521 - acc: 0.5642\n",
      "Epoch 18/50\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 19/50\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 20/50\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7521 - acc: 0.5642\n",
      "Epoch 21/50\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 22/50\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 23/50\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 24/50\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 25/50\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 26/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 27/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 28/50\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 29/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 30/50\n",
      "5948/5948 [==============================] - 0s 69us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 31/50\n",
      "5948/5948 [==============================] - 0s 72us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 32/50\n",
      "5948/5948 [==============================] - 0s 84us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 33/50\n",
      "5948/5948 [==============================] - 0s 57us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 34/50\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 35/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 36/50\n",
      "5948/5948 [==============================] - 0s 63us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 37/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 38/50\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 39/50\n",
      "5948/5948 [==============================] - 0s 51us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 40/50\n",
      "5948/5948 [==============================] - 0s 45us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 41/50\n",
      "5948/5948 [==============================] - 0s 51us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 42/50\n",
      "5948/5948 [==============================] - 0s 47us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 43/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 44/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 45/50\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 46/50\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 47/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 48/50\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 49/50\n",
      "5948/5948 [==============================] - 0s 57us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 50/50\n",
      "5948/5948 [==============================] - 0s 53us/step - loss: 0.7511 - acc: 0.5642\n",
      "1487/1487 [==============================] - 0s 74us/step\n",
      "test loss\n",
      "0.7079899625723685\n",
      "test_acc\n",
      "0.6146603902875776\n",
      "Epoch 1/50\n",
      "5948/5948 [==============================] - 0s 67us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 2/50\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 3/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 4/50\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 5/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 6/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 7/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 8/50\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 9/50\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 10/50\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 11/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 12/50\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 13/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 14/50\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 15/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 16/50\n",
      "5948/5948 [==============================] - 0s 45us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 17/50\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 18/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 19/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 20/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 21/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 22/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 23/50\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 24/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 25/50\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 26/50\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 27/50\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 28/50\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 29/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 30/50\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 31/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 33/50\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 34/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 35/50\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 36/50\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 37/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 38/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 39/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 40/50\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 41/50\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 42/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 43/50\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 44/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 45/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 46/50\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 47/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 48/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 49/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 50/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7513 - acc: 0.5642\n",
      "1487/1487 [==============================] - 0s 64us/step\n",
      "test loss\n",
      "0.7130034879781932\n",
      "test_acc\n",
      "0.6146603902875776\n",
      "Epoch 1/50\n",
      "5948/5948 [==============================] - 0s 73us/step - loss: 0.7519 - acc: 0.5642\n",
      "Epoch 2/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 3/50\n",
      "5948/5948 [==============================] - 0s 45us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 4/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 5/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 6/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 7/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 8/50\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 9/50\n",
      "5948/5948 [==============================] - 0s 52us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 10/50\n",
      "5948/5948 [==============================] - 0s 66us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 11/50\n",
      "5948/5948 [==============================] - 0s 45us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 12/50\n",
      "5948/5948 [==============================] - 0s 51us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 13/50\n",
      "5948/5948 [==============================] - 0s 60us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 14/50\n",
      "5948/5948 [==============================] - 0s 54us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 15/50\n",
      "5948/5948 [==============================] - 0s 67us/step - loss: 0.7506 - acc: 0.5642\n",
      "Epoch 16/50\n",
      "5948/5948 [==============================] - 0s 47us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 17/50\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 18/50\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 19/50\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 20/50\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 21/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 22/50\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 23/50\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7506 - acc: 0.5642\n",
      "Epoch 24/50\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 25/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 26/50\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 27/50\n",
      "5948/5948 [==============================] - 0s 35us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 28/50\n",
      "5948/5948 [==============================] - 0s 35us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 29/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 30/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 31/50\n",
      "5948/5948 [==============================] - 0s 45us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 32/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 33/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 34/50\n",
      "5948/5948 [==============================] - 0s 44us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 35/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 36/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 37/50\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 38/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 39/50\n",
      "5948/5948 [==============================] - 0s 35us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 40/50\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 41/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 42/50\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 43/50\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 44/50\n",
      "5948/5948 [==============================] - 0s 35us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 45/50\n",
      "5948/5948 [==============================] - 0s 35us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 46/50\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 47/50\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 48/50\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 49/50\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 50/50\n",
      "5948/5948 [==============================] - 0s 34us/step - loss: 0.7507 - acc: 0.5642\n",
      "1487/1487 [==============================] - 0s 59us/step\n",
      "test loss\n",
      "0.7078841472922673\n",
      "test_acc\n",
      "0.6146603902875776\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(76, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(60, activation=tf.nn.softmax),\n",
    "    keras.layers.Dense(40, activation=keras.activations.linear),\n",
    "    keras.layers.Dense(10, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(8, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "#after searching for learning rates, 0.02 was found to be the optimal \n",
    "#after searching for epochs, 50 was found to be the most optimal \n",
    "\n",
    "for i in ['sparse_categorical_crossentropy']:\n",
    "    for j in [tf.train.AdamOptimizer(), tf.train.GradientDescentOptimizer(learning_rate=0.02),\n",
    "             tf.train.AdagradOptimizer(learning_rate = 0.02)]:\n",
    "        model.compile(optimizer=j, \n",
    "              loss=i,\n",
    "              metrics=['accuracy'])\n",
    "        model.fit(np.asarray(x_tra), y_tra, epochs=50)\n",
    "        test_loss, test_acc = model.evaluate(np.asarray(x_dev), y_dev)\n",
    "        print(\"test loss\")\n",
    "        print(test_loss)\n",
    "        print(\"test_acc\")\n",
    "        print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5948/5948 [==============================] - 0s 83us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 2/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 3/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 4/100\n",
      "5948/5948 [==============================] - 0s 44us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 5/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 6/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 7/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 8/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 9/100\n",
      "5948/5948 [==============================] - 0s 45us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 10/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 11/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 12/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 13/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 14/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 15/100\n",
      "5948/5948 [==============================] - 0s 44us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 16/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 17/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 18/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 19/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 20/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 21/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 22/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 23/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 24/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 25/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 26/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 27/100\n",
      "5948/5948 [==============================] - 0s 44us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 28/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 29/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 30/100\n",
      "5948/5948 [==============================] - 0s 47us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 31/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 32/100\n",
      "5948/5948 [==============================] - 0s 68us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 33/100\n",
      "5948/5948 [==============================] - 0s 74us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 34/100\n",
      "5948/5948 [==============================] - 0s 61us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 35/100\n",
      "5948/5948 [==============================] - 0s 72us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 36/100\n",
      "5948/5948 [==============================] - 0s 46us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 37/100\n",
      "5948/5948 [==============================] - 0s 46us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 38/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 39/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 40/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 41/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 42/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 43/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 44/100\n",
      "5948/5948 [==============================] - 0s 44us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 45/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 46/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 47/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 48/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 49/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 50/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 51/100\n",
      "5948/5948 [==============================] - 0s 50us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 52/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 53/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 54/100\n",
      "5948/5948 [==============================] - 0s 64us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 55/100\n",
      "5948/5948 [==============================] - 0s 49us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 56/100\n",
      "5948/5948 [==============================] - 0s 44us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 57/100\n",
      "5948/5948 [==============================] - 0s 44us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 58/100\n",
      "5948/5948 [==============================] - 0s 48us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 59/100\n",
      "5948/5948 [==============================] - 0s 59us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 60/100\n",
      "5948/5948 [==============================] - 0s 68us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 61/100\n",
      "5948/5948 [==============================] - 0s 49us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 62/100\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 63/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 64/100\n",
      "5948/5948 [==============================] - 0s 49us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 65/100\n",
      "5948/5948 [==============================] - 0s 48us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 66/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7506 - acc: 0.5642\n",
      "Epoch 67/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 68/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7514 - acc: 0.5642\n",
      "Epoch 69/100\n",
      "5948/5948 [==============================] - 0s 44us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 70/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 71/100\n",
      "5948/5948 [==============================] - 0s 67us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 72/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 73/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 74/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 75/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 76/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 77/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 78/100\n",
      "5948/5948 [==============================] - 0s 59us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 79/100\n",
      "5948/5948 [==============================] - 1s 121us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 80/100\n",
      "5948/5948 [==============================] - 0s 64us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 81/100\n",
      "5948/5948 [==============================] - 1s 118us/step - loss: 0.7507 - acc: 0.5642 0s - loss: 0.7540 - acc:\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5948/5948 [==============================] - 0s 72us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 83/100\n",
      "5948/5948 [==============================] - 0s 67us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 84/100\n",
      "5948/5948 [==============================] - 0s 83us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 85/100\n",
      "5948/5948 [==============================] - 1s 89us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 86/100\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 87/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 88/100\n",
      "5948/5948 [==============================] - 0s 47us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 89/100\n",
      "5948/5948 [==============================] - 0s 45us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 90/100\n",
      "5948/5948 [==============================] - 0s 51us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 91/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 92/100\n",
      "5948/5948 [==============================] - 0s 46us/step - loss: 0.7506 - acc: 0.5642\n",
      "Epoch 93/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 94/100\n",
      "5948/5948 [==============================] - 1s 93us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 95/100\n",
      "5948/5948 [==============================] - 0s 59us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 96/100\n",
      "5948/5948 [==============================] - 0s 71us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 97/100\n",
      "5948/5948 [==============================] - 0s 73us/step - loss: 0.7516 - acc: 0.5642\n",
      "Epoch 98/100\n",
      "5948/5948 [==============================] - 0s 77us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 99/100\n",
      "5948/5948 [==============================] - 0s 49us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 100/100\n",
      "5948/5948 [==============================] - 0s 51us/step - loss: 0.7510 - acc: 0.5642\n",
      "1487/1487 [==============================] - 0s 77us/step\n",
      "test loss\n",
      "0.707438414258771\n",
      "test_acc\n",
      "0.6146603902875776\n",
      "Epoch 1/100\n",
      "5948/5948 [==============================] - 0s 75us/step - loss: 0.7521 - acc: 0.5614\n",
      "Epoch 2/100\n",
      "5948/5948 [==============================] - 0s 49us/step - loss: 0.7515 - acc: 0.5620\n",
      "Epoch 3/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7530 - acc: 0.5619\n",
      "Epoch 4/100\n",
      "5948/5948 [==============================] - 0s 45us/step - loss: 0.7524 - acc: 0.5642\n",
      "Epoch 5/100\n",
      "5948/5948 [==============================] - 0s 51us/step - loss: 0.7529 - acc: 0.5642\n",
      "Epoch 6/100\n",
      "5948/5948 [==============================] - 0s 47us/step - loss: 0.7525 - acc: 0.5642\n",
      "Epoch 7/100\n",
      "5948/5948 [==============================] - 0s 45us/step - loss: 0.7521 - acc: 0.5627\n",
      "Epoch 8/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7521 - acc: 0.5614\n",
      "Epoch 9/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7528 - acc: 0.5622\n",
      "Epoch 10/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7525 - acc: 0.5590\n",
      "Epoch 11/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7528 - acc: 0.5642\n",
      "Epoch 12/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7531 - acc: 0.5636\n",
      "Epoch 13/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7530 - acc: 0.5642\n",
      "Epoch 14/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7518 - acc: 0.5620\n",
      "Epoch 15/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7536 - acc: 0.5642\n",
      "Epoch 16/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7529 - acc: 0.5642\n",
      "Epoch 17/100\n",
      "5948/5948 [==============================] - 0s 50us/step - loss: 0.7527 - acc: 0.5632\n",
      "Epoch 18/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7523 - acc: 0.5642\n",
      "Epoch 19/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7519 - acc: 0.5642\n",
      "Epoch 20/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7526 - acc: 0.5642\n",
      "Epoch 21/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7529 - acc: 0.5642\n",
      "Epoch 22/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7523 - acc: 0.5607\n",
      "Epoch 23/100\n",
      "5948/5948 [==============================] - 0s 45us/step - loss: 0.7524 - acc: 0.5642\n",
      "Epoch 24/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7525 - acc: 0.5609\n",
      "Epoch 25/100\n",
      "5948/5948 [==============================] - 0s 47us/step - loss: 0.7520 - acc: 0.5642\n",
      "Epoch 26/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7522 - acc: 0.5602\n",
      "Epoch 27/100\n",
      "5948/5948 [==============================] - 0s 48us/step - loss: 0.7532 - acc: 0.5642: 0s - loss: 0.7555 - acc: 0.565\n",
      "Epoch 28/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7530 - acc: 0.5642\n",
      "Epoch 29/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7522 - acc: 0.5642\n",
      "Epoch 30/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7519 - acc: 0.5639\n",
      "Epoch 31/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7518 - acc: 0.5634\n",
      "Epoch 32/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7517 - acc: 0.5637\n",
      "Epoch 33/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7525 - acc: 0.5642\n",
      "Epoch 34/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7529 - acc: 0.5642\n",
      "Epoch 35/100\n",
      "5948/5948 [==============================] - 0s 47us/step - loss: 0.7516 - acc: 0.5636\n",
      "Epoch 36/100\n",
      "5948/5948 [==============================] - 0s 46us/step - loss: 0.7523 - acc: 0.5629\n",
      "Epoch 37/100\n",
      "5948/5948 [==============================] - 0s 55us/step - loss: 0.7520 - acc: 0.5642\n",
      "Epoch 38/100\n",
      "5948/5948 [==============================] - 0s 51us/step - loss: 0.7522 - acc: 0.5642\n",
      "Epoch 39/100\n",
      "5948/5948 [==============================] - 0s 51us/step - loss: 0.7527 - acc: 0.5642\n",
      "Epoch 40/100\n",
      "5948/5948 [==============================] - 0s 58us/step - loss: 0.7527 - acc: 0.5642\n",
      "Epoch 41/100\n",
      "5948/5948 [==============================] - 0s 58us/step - loss: 0.7524 - acc: 0.5625\n",
      "Epoch 42/100\n",
      "5948/5948 [==============================] - 0s 68us/step - loss: 0.7528 - acc: 0.5630\n",
      "Epoch 43/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7527 - acc: 0.5642\n",
      "Epoch 44/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7522 - acc: 0.5642\n",
      "Epoch 45/100\n",
      "5948/5948 [==============================] - 0s 68us/step - loss: 0.7530 - acc: 0.5629\n",
      "Epoch 46/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7529 - acc: 0.5642\n",
      "Epoch 47/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7528 - acc: 0.5622\n",
      "Epoch 48/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7521 - acc: 0.5615\n",
      "Epoch 49/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7526 - acc: 0.5625\n",
      "Epoch 50/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7529 - acc: 0.5642\n",
      "Epoch 51/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7529 - acc: 0.5642\n",
      "Epoch 52/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7525 - acc: 0.5642\n",
      "Epoch 53/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7525 - acc: 0.5627\n",
      "Epoch 54/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7520 - acc: 0.5622\n",
      "Epoch 55/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7537 - acc: 0.5642\n",
      "Epoch 56/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7524 - acc: 0.5629\n",
      "Epoch 57/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7523 - acc: 0.5642\n",
      "Epoch 58/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7522 - acc: 0.5629\n",
      "Epoch 59/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7523 - acc: 0.5639\n",
      "Epoch 60/100\n",
      "5948/5948 [==============================] - 0s 56us/step - loss: 0.7529 - acc: 0.5642\n",
      "Epoch 61/100\n",
      "5948/5948 [==============================] - 0s 48us/step - loss: 0.7529 - acc: 0.5642\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7522 - acc: 0.5627\n",
      "Epoch 63/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7529 - acc: 0.5642\n",
      "Epoch 64/100\n",
      "5948/5948 [==============================] - 0s 35us/step - loss: 0.7519 - acc: 0.5642\n",
      "Epoch 65/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7532 - acc: 0.5642\n",
      "Epoch 66/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7526 - acc: 0.5642\n",
      "Epoch 67/100\n",
      "5948/5948 [==============================] - 0s 34us/step - loss: 0.7523 - acc: 0.5642\n",
      "Epoch 68/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 69/100\n",
      "5948/5948 [==============================] - 0s 35us/step - loss: 0.7527 - acc: 0.5642\n",
      "Epoch 70/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7523 - acc: 0.5639\n",
      "Epoch 71/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7529 - acc: 0.5642\n",
      "Epoch 72/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7519 - acc: 0.5642\n",
      "Epoch 73/100\n",
      "5948/5948 [==============================] - 0s 34us/step - loss: 0.7523 - acc: 0.5617\n",
      "Epoch 74/100\n",
      "5948/5948 [==============================] - 0s 33us/step - loss: 0.7526 - acc: 0.5642\n",
      "Epoch 75/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7527 - acc: 0.5615\n",
      "Epoch 76/100\n",
      "5948/5948 [==============================] - 0s 35us/step - loss: 0.7511 - acc: 0.5604\n",
      "Epoch 77/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7519 - acc: 0.5642\n",
      "Epoch 78/100\n",
      "5948/5948 [==============================] - 0s 34us/step - loss: 0.7517 - acc: 0.5610\n",
      "Epoch 79/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7529 - acc: 0.5629\n",
      "Epoch 80/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7518 - acc: 0.5620\n",
      "Epoch 81/100\n",
      "5948/5948 [==============================] - 0s 34us/step - loss: 0.7527 - acc: 0.5642\n",
      "Epoch 82/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7530 - acc: 0.5627\n",
      "Epoch 83/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7525 - acc: 0.5642\n",
      "Epoch 84/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7524 - acc: 0.5642\n",
      "Epoch 85/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7526 - acc: 0.5588\n",
      "Epoch 86/100\n",
      "5948/5948 [==============================] - 0s 35us/step - loss: 0.7524 - acc: 0.5627\n",
      "Epoch 87/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7521 - acc: 0.5632\n",
      "Epoch 88/100\n",
      "5948/5948 [==============================] - 0s 49us/step - loss: 0.7519 - acc: 0.5614\n",
      "Epoch 89/100\n",
      "5948/5948 [==============================] - 0s 45us/step - loss: 0.7528 - acc: 0.5615\n",
      "Epoch 90/100\n",
      "5948/5948 [==============================] - 0s 59us/step - loss: 0.7524 - acc: 0.5642\n",
      "Epoch 91/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7529 - acc: 0.5634\n",
      "Epoch 92/100\n",
      "5948/5948 [==============================] - 0s 44us/step - loss: 0.7525 - acc: 0.5642\n",
      "Epoch 93/100\n",
      "5948/5948 [==============================] - 0s 45us/step - loss: 0.7526 - acc: 0.5642\n",
      "Epoch 94/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7527 - acc: 0.5632\n",
      "Epoch 95/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7528 - acc: 0.5642\n",
      "Epoch 96/100\n",
      "5948/5948 [==============================] - 0s 34us/step - loss: 0.7519 - acc: 0.5632\n",
      "Epoch 97/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7523 - acc: 0.5625\n",
      "Epoch 98/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 99/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7524 - acc: 0.5624\n",
      "Epoch 100/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7519 - acc: 0.5642\n",
      "1487/1487 [==============================] - 0s 80us/step\n",
      "test loss\n",
      "0.707392533152554\n",
      "test_acc\n",
      "0.6146603902875776\n",
      "Epoch 1/100\n",
      "5948/5948 [==============================] - 0s 73us/step - loss: 0.7536 - acc: 0.5642\n",
      "Epoch 2/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7518 - acc: 0.5642\n",
      "Epoch 3/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7517 - acc: 0.5642\n",
      "Epoch 4/100\n",
      "5948/5948 [==============================] - 0s 45us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 5/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 6/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7515 - acc: 0.5642\n",
      "Epoch 7/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 8/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 9/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7512 - acc: 0.5642\n",
      "Epoch 10/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 11/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 12/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 13/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7513 - acc: 0.5642\n",
      "Epoch 14/100\n",
      "5948/5948 [==============================] - 0s 35us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 15/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 16/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 17/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 18/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 19/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 20/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 21/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 22/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 23/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 24/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 25/100\n",
      "5948/5948 [==============================] - 0s 44us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 26/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 27/100\n",
      "5948/5948 [==============================] - 0s 35us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 28/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 29/100\n",
      "5948/5948 [==============================] - 0s 35us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 30/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 31/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 32/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 33/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 34/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 35/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 36/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 37/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7511 - acc: 0.5642\n",
      "Epoch 38/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 39/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 40/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 41/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 42/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7506 - acc: 0.5642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "5948/5948 [==============================] - 0s 45us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 44/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 45/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 46/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 47/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 48/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 49/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 50/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 51/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7506 - acc: 0.5642\n",
      "Epoch 52/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 53/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 54/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 55/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 56/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 57/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 58/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 59/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 60/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 61/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 62/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7510 - acc: 0.5642\n",
      "Epoch 63/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 64/100\n",
      "5948/5948 [==============================] - 0s 44us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 65/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 66/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 67/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 68/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 69/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 70/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 71/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 72/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 73/100\n",
      "5948/5948 [==============================] - 0s 35us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 74/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 75/100\n",
      "5948/5948 [==============================] - 0s 46us/step - loss: 0.7506 - acc: 0.5642\n",
      "Epoch 76/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 77/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 78/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 79/100\n",
      "5948/5948 [==============================] - 0s 36us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 80/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 81/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 82/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 83/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 84/100\n",
      "5948/5948 [==============================] - 0s 42us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 85/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 86/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 87/100\n",
      "5948/5948 [==============================] - 0s 37us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 88/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 89/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 90/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 91/100\n",
      "5948/5948 [==============================] - 0s 39us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 92/100\n",
      "5948/5948 [==============================] - 0s 45us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 93/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 94/100\n",
      "5948/5948 [==============================] - 0s 48us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 95/100\n",
      "5948/5948 [==============================] - 0s 41us/step - loss: 0.7509 - acc: 0.5642\n",
      "Epoch 96/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7507 - acc: 0.5642\n",
      "Epoch 97/100\n",
      "5948/5948 [==============================] - 0s 40us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 98/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 99/100\n",
      "5948/5948 [==============================] - 0s 43us/step - loss: 0.7508 - acc: 0.5642\n",
      "Epoch 100/100\n",
      "5948/5948 [==============================] - 0s 38us/step - loss: 0.7508 - acc: 0.5642\n",
      "1487/1487 [==============================] - 0s 73us/step\n",
      "test loss\n",
      "0.7079509022096379\n",
      "test_acc\n",
      "0.6146603902875776\n"
     ]
    }
   ],
   "source": [
    "for i in ['sparse_categorical_crossentropy']:\n",
    "    for j in [tf.train.AdamOptimizer(), tf.train.GradientDescentOptimizer(learning_rate=0.05),\n",
    "             tf.train.AdagradOptimizer(learning_rate = 0.05)]:\n",
    "        model.compile(optimizer=j, \n",
    "              loss=i,\n",
    "              metrics=['accuracy'])\n",
    "        model.fit(np.asarray(x_tra), y_tra, epochs=100)\n",
    "        test_loss, test_acc = model.evaluate(np.asarray(x_dev), y_dev)\n",
    "        print(\"test loss\")\n",
    "        print(test_loss)\n",
    "        print(\"test_acc\")\n",
    "        print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
